---
output: word_document
---
#1 Bike Sharing System

##1.1 Background
Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, users are able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. 

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.

##1.2 Dataset
Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. We aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information. Weather information are extracted from http://www.freemeteo.com. 

##1.3 Dataset Characteristics
Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv
	
	- instant: record index
	- dteday : date
	- season : season (1:springer, 2:summer, 3:fall, 4:winter)
	- yr : year (0: 2011, 1:2012)
	- mnth : month ( 1 to 12)
	- hr : hour (0 to 23)
	- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
	- weekday : day of the week
	- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
	+ weathersit : 
		- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
		- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
		- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
		- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
	- temp : Normalized temperature in Celsius. The values are divided to 41 (max)
	- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)
	- hum: Normalized humidity. The values are divided to 100 (max)
	- windspeed: Normalized wind speed. The values are divided to 67 (max)
	- casual: count of casual users
	- registered: count of registered users
	- cnt: count of total rental bikes including both casual and registered

##1.4 License
Use of this dataset in publications must be cited to the following publication:

[1] Fanaee-T, Hadi, and Gama, Joao, "Event labeling combining ensemble detectors and background knowledge", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.

@article{
	year={2013},
	issn={2192-6352},
	journal={Progress in Artificial Intelligence},
	doi={10.1007/s13748-013-0040-3},
	title={Event labeling combining ensemble detectors and background knowledge},
	url={http://dx.doi.org/10.1007/s13748-013-0040-3},
	publisher={Springer Berlin Heidelberg},
	keywords={Event labeling; Event detection; Ensemble learning; Background knowledge},
	author={Fanaee-T, Hadi and Gama, Joao},
	pages={1-15}
}


#2 Data Exploration
We started off by reading the data set stored in "day.csv" file into a dataframe is R using the following command.
```{r}
bikerental<-read.csv("C:/Users/Akash/Desktop/DAM Project/Dataset/day.csv",header = TRUE)
head(bikerental)
```
We can see from the above result the data has been loaded into the dataset bikerental.

##2.1 Number of observations in the dataset.
We use the following commands to get the count of the number of observations present in the dataset.
```{r}
nrow(bikerental)
```
We can see from the result above that there are 731 observations in the dataset. As we know from the data charcteristic that the whole data is divided into the observations collected over 2 years. Hence we will divide the dataset into two subsets, one for model building and one to test the prediction.
We use the following command to divide the dataset into two subsets.
```{r}
bikerentalyear1<-subset(bikerental, bikerental$yr == 0)
bikerentalyear2<-subset(bikerental, bikerental$yr == 1)
```
Next we will count the total observations for each of the year.
```{r}
nrow(bikerentalyear1)
nrow(bikerentalyear2)
```
We can see from the ouput of the above commands that we have 365 observations for 2011 i.e. our training set and we have 366 observation for 2012 which is our prediction set.

##2.2 Number of columns in the dataset and their characteristics.
We use the following commands to get the count of the number of columns(variables) present in the dataset.
```{r}
ncol(bikerental)
summary(bikerental)
```
We can see from the result above that there are 16 columns(variables) in the dataset. Out of these columns, cnt is our response variable and the rest are our covariates. 

##2.3 Check for null values in the dataset.
We will use the following command to check whether null values are present in to dataset or not.
```{r}
nacheck<-is.na(bikerental)
sum(nacheck)
```
We can see from the above results that there are no missing values in the dataset.

##2.4 Converting values from normalized form to actual form.
In out dataset, as explaned in the data characteristic, we have certain variables which are normalized. These variables are "temp", "atemp", "hum" and "windspeed". We now convert the variables to their actual values using the following code.
```{r}
#changing normalized values of actaul temperature to actual values:
bikerentalyear1$actualtemp <- bikerentalyear1$temp*41
#changing feeled temperature to actual values:
bikerentalyear1$feeltemp <- bikerentalyear1$atemp*50
#changing humidity to actual values:
bikerentalyear1$actualhum <- bikerentalyear1$hum*100
#changing windspeed to actual values:
bikerentalyear1$actualwind <- bikerentalyear1$windspeed*67
```

##2.5 Checking colleration between variables in the dataset.
We will use the following function to check the correlation between the variables.
```{r}
cor(bikerentalyear1[3:20])
```
*From the correlation matrix, we can infer the following:*  
* Count(cnt) is correlated with temp, atemp, season, month, weathersit, casual, registered and windspeed. *since   
* temp and atemp are highly correlated with each other and hence we will include only one of them into our model to avoid multicollinearity. We are including actualtemp(actual value of temp)  
* season and month are highly correlated with each other and hence we will include only one of them into our model to avoid multicollinearity. We are including season  
* casual, registered and count are higly correlated. Single count(cnt) is basically the sum of casual and registered, there is no need to include them into the model.

#3 Data Visualization

Correlation heatmap for bikerentalyear1:
```{r}
m <- data.frame(bikerentalyear1[,3:16] )
library(ggplot2)
library(reshape2)
qplotdate <- qplot(x=Var1, y= Var2, data=melt(cor(m)), fill=value, geom="tile")
qplotdate+scale_fill_gradient(low="yellow", high="red")+theme(axis.text.x=element_text(angle=90,hjust = 1)) 
```
The result of the above heatmap concurs with the results in section 2.5.

Plotting temperatures for different seasons:
```{r}
#temperature ranges for each season:
springer1 <- subset(bikerentalyear1,season == 1)
smean1 <- mean(springer1$atemp)
sstd1 <- sd(springer1$atemp)

summer <- subset(bikerentalyear1,season == 2)
smean2 <- mean(summer$atemp)
sstd2 <- sd(summer$atemp)

fall <- subset(bikerentalyear1,season == 3)
smean3 <- mean(fall$atemp)
sstd3 <- sd(fall$atemp)

winter <- subset(bikerentalyear1,season == 4)
smean4 <- mean(winter$atemp)
sstd4 <- sd(winter$atemp)

par(mfrow = c(2,2))
hist(springer1$atemp, main = "Spring Temperature Histogram", xlab = "Temperature", col = blues9 )
hist(summer$atemp, main = "Summer Temperature Histogram", xlab = "Temperature", col = blues9 )
hist(fall$atemp, main = "Fall Temperature Histogram", xlab = "Temperature", col = blues9 )
hist(winter$atemp, main = "Winter Temperature Histogram", xlab = "Temperature", col = blues9 )
bikerentalyear1$season <- as.factor(bikerentalyear1$season)
plot(bikerentalyear1$season,bikerentalyear1$cnt, col = blues9)
```
We can see from the histograms and the mean values of the temperature for different seasons that the temperature variations for different season are in order: Fall, Summer, Winter and then spring.

#4 Model Building
##4.1 Selection of covariates
From section 2.5, we narrowed down our covariates to actualtemp, season, weathersit, windspeed and hum. Since season and weathersit are vategorical variables, we will convert them to factors using the following code:
```{r}
bikerentalyear1$weatherfac <- as.factor(bikerentalyear1$weathersit)
bikerentalyear1$seasonfac <- as.factor(bikerentalyear1$season)
head(bikerentalyear1)
```

So our new list of covarites are: actualtemp, weatherfac, seasonfac, windspeed and hum.

##4.2 Model Building
We will start with bulding a multiple linear regression model by taking the covariates confirmed in section 4.1 and taking the response variable as cnt(count).  
To build the multiple linear regression model, we will use the following command:
```{r}
attach(bikerentalyear1)
rentalmodel <- lm(cnt~actualtemp+seasonfac+weatherfac+windspeed+hum, data = bikerentalyear1)
summary(rentalmodel)
```
From the above results, we can see that the 'rentalmodel' has been created and the fit of the model is quite satisfactory.

##4.3 Hypothesis testing and Partial Testing
To check if the all the coviates are significant in determining the reponse variable and to check the overall adequecy of the model, we will perform Hypothesis test and Partial tests.

###4.3.1 Checking overall adequecy of the model
To check the adequecy of the model, we will perform the test for significance to test if there is a linear relationship between the response variable and any of the covariates. To perform the test of significance, we use the F-test.

To perform the F-test we first form the the null (*$H_0$*) and alternate hypothesis (*$H_1$*), which are as follows:  
**$H_0$:There is no linear relation between response variable and covarites**  
**$H_1$:There is a linear relation between response variable and covariates**  
*Explanation: In out Null hypothesis, we assume that there is no collective effect of our covariates on the response variable. Our alternate hypothesis states that our covariates collectively have an effect on the response variable.* 
We execute the following command to get the F-test results, also known as the F-stats for this model:  
```{r}
anova(rentalmodel)
```

The output of the above query shows that for each of the covariate *p-value < alpha(0.05)*, which clearly indicates that we should reject the null hypothesis at the 5% level of significance, that the covariates collectively have no effect on the response variable.

###4.3.2 Hypothesis test for estimate coefficients

To perform t-tests for each of the regression coefficient estimate, we will have to perform the following steps:  

* Form the null hypothesis- $H_0$: $\beta_i$ = 0, where i = [1,8]  
* Form the alternate hypothesis - $H_1$: $\beta_i$ != 0, where i = [1,8]  
* Obtain t-stat for $\beta_i$  
* Evaluate the significance of each of the regression coefficient.  

*Note*: $H_0$: $\beta_i$ = 0, where i = [1,8], the t-test is basically being done to determine whether the  regression coefficient is significant or not and hence in null hypothesis we state that the significance of the regression coefficient is negligible.

To obtain the t-stat for each of the coefficient, we can execute the following commands:
```{r}
summary(rentalmodel)$coef[,3:4]
```
For the results above, below things can be said about the regression coefficients at 5% level of significance:  

* $\beta_0$(regression coefficient for intercept), is significant because the p-value(3.857e-13) < alpha(0.05) and hence the null hypothesis($\beta_0$ = 0 i.e. intercept being zero) will be rejected
* $\beta_1$(regression coefficient for actualtemp), is not significant because the p-value(1.15e-34) < alpha(0.05) and hence the null hypothesis($\beta_1$ = 0 i.e. the effect of actualtemp on cnt is insignificant) will be rejected
* $\beta_2$(regression coefficient for seasonfac2), is not significant because the p-value(5.865e-17) < alpha(0.05) and hence the null hypothesis($\beta_2$ = 0 i.e. the effect of seasonfac2 on cnt is insignificant) will not be rejected
* $\beta_3$(regression coefficient for seasonfac3), is significant because the p-value(7.40e-08) < alpha(0.05) and hence the null hypothesis($\beta_3$ = 0 i.e. the effect of seasonfac3 on cnt is insignificant) will be rejected
* $\beta_4$(regression coefficient for seasonfac4), is significant because the p-value(1.25e-33) < alpha(0.05) and hence the null hypothesis($\beta_4$ = 0 i.e. the effect of seasonfac4 on cnt is significant) will be rejected
* $\beta_5$(regression coefficient for weatherfac2), is significant because the p-value(3.818e-04) < alpha(0.05) and hence the null hypothesis($\beta_5$ = 0 i.e. the effect of weatherfac2 on cnt is insignificant) will be rejected
* $\beta_6$(regression coefficient for weatherfac3), is significant because the p-value(1.217e-16) < alpha(0.05) and hence the null hypothesis($\beta_6$ = 0 i.e. the effect of weatherfac3 on cnt is insignificant) will be rejected
* $\beta_7$(regression coefficient for windspeed), is significant because the p-value(9.077e-07) < alpha(0.05) and hence the null hypothesis($\beta_7$ = 0 i.e. the effect of windspeed on cnt is insignificant) will be rejected
* $\beta_8$(regression coefficient for hum), is significant because the p-value(2.084e-03) < alpha(0.05) and hence the null hypothesis($\beta_8$ = 0 i.e. the effect of hum on cnt is insignificant) will be rejected


#5 Taking remaining variables into consideration
Till now we have taken actualtemp, weatherfac, seasonfac, windspeed and hum into consideration. Some variables like mnth, atemp, casual and registered were rejected based on section 2.5. The remaining variables to be taken into consideration are: holiday, weekday and workingday.

We will check each of the remaining variables by adding them to the rentalmodel and apply F-test to check their significance.

##5.1 Check for holiday
We will make a new model "rentalmodelholi" by adding holiday to the previous model and then apply F-test to it.
```{r}
rentalmodelholi <- lm(cnt~actualtemp+seasonfac+weatherfac+windspeed+hum+holiday, data = bikerentalyear1)
anova(rentalmodel,rentalmodelholi)
```
From the above result, we can see that the p-value(.06) > alpha (.05) and we fail reject the null hypothesis that holiday has no effect on cnt. Hence we will not add holiday to the model.

##5.2 Check for weekday
We will make a new model "rentalmodelweekday" by adding holiday to the previous model and then apply F-test to it.
```{r}
rentalmodelweekday <- lm(cnt~actualtemp+seasonfac+weatherfac+windspeed+hum+weekday, data = bikerentalyear1)
anova(rentalmodel,rentalmodelweekday)
```
From the above result, we can see that the p-value(.12) > alpha (.05) and we fail reject the null hypothesis that weekday has no effect on cnt. Hence we will not add weekday to the model.

##5.2 Check for workingday
We will make a new model "rentalmodelworking" by adding holiday to the previous model and then apply F-test to it.
```{r}
rentalmodelworking <- lm(cnt~actualtemp+seasonfac+weatherfac+windspeed+hum+workingday, data = bikerentalyear1)
anova(rentalmodel,rentalmodelworking)
```
From the above result, we can see that the p-value(.41) > alpha (.05) and we fail reject the null hypothesis that weekday has no effect on cnt. Hence we will not add workingday to the model.

_*Hence our final list of covariates are: actualtemp, seasonfac, weatherfac, windspeed, hum.*_

#6 Analysis of Residuals
Since "rentalmodel" is our final model, we will analyze the residuals to check whether they are randomly distributed around zero or not.
```{r}
par(mfrow=c(1,2))
plot(rentalmodel$fitted.values,rentalmodel$residuals, xlab = "Fitted Values", ylab = "Residuals", main = "Residuals vs Fitted values", col = blues9)
abline(h=0, col = "red")
qqnorm(rentalmodel$residuals, col = blues9)
qqline(rentalmodel$residuals, col = "red")
```

From the above graphs, we can conclude that the residuals are random normally distributed with some outliers.

#7 Transforming the model using BoxCox Method
From section 6, it is clear that the model is a good fit with random normally distributed residuals. Now in order to get a good fit with less residual standard error we will apply the boxcox transformation to out model.
```{r}
par(mfrow=c(1,1))
library(MASS,quietly = TRUE)

```
From the above graph, the value of lamda should be between .8 to .99. We tried all the combinations and the best result we got was with lamda = .95. To apply the transformation, we will execute the following code.
```{r}
bikerentalyear1$transcnt <- bikerentalyear1$cnt*.95
rentalmodelFinal<-lm(transcnt~actualtemp+weatherfac+hum+windspeed+seasonfac, data=bikerentalyear1)
summary(rentalmodelFinal)
summary(rentalmodel)
```
From the above result, we can see that the transformation didnot change the fit of the model but it reduced the Residual standard error and hence we will accept the updated model.

#8 Using model to predict the future values
In section 2, we divided the data into two sets, one for model building and one for prediction testing. Now we will use out final model to get the prediction values for the next year and compare them with the actual observed values.
We will use the following commands to perform the pridiction:
```{r}
bikerentalyear2$transcnt<-bikerentalyear2$cnt^.95
bikeyear2<-predict(rentalmodelFinal, interval =  "predict")
plot(bikeyear2[,1],type = "p",col="green", ylab = "RentalCount")
points(bikeyear2[,2],type = "p",col="red")
points(bikeyear2[,3],type = "p",col="red")
points(bikerentalyear2$transcnt, type="p", pch = 20 , col = "blue")
```
From the above graph, we can see that our final model was able to predict the values for the rental count for the next year quite accurately.

#9 Additional Insights
Since our count of rental is divided into casual and registered. We are interested in finding the trend of temperature with bike rentals for registered and casual users. To do that, we execute te foloowing code:
```{r}

```
From the above result, we can conclude that the count of Rental Bikes increase with temperature and this increase is more drastic for registered users as compared to that for casual users. 

#10 Conclusions
Following conclusions can be made from the above analysis:  
* Bike Rental is highly dependent of temperature
* Bike Rental in all depends on the actual temperature, season, Weather, humidity and windspeed
* Bike Rental count is maximum in fall because all the above parameters are optimal in fall
* Bike Rental increases more rapidly with temperature for registered users as compared to that for casual users.